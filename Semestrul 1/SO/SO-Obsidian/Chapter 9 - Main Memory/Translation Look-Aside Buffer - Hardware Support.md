
- The hardware implementation of the page table can be done in several ways. In the simplest case, the page table is implemented as a set of dedicated high-speed hardware registers, which makes the page-address translation very efficient. However, this approach increases context-switch time, as each one of these registers must be exchanged during a context switch. 
	- The use of registers for the page table is satisfactory if the page table is reasonably small (for example, 256 entries). Most contemporary CPUs, however, support much larger page tables (for example, 220 entries). For these machines, the use of fast registers to implement the page table is not feasible. Rather, the page table is kept in main memory, and a <span style="color:rgb(112, 48, 160)">page-table base register</span> (<span style="color:rgb(112, 48, 160)">PTBR</span>) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time.
- Although <span style="color:rgb(112, 48, 160)">storing the page table in main memory</span> can yield faster context switches, it may also result in slower memory access times. Suppose we want to access location i. We must first index into the page table, using the value in the <span style="color:rgb(112, 48, 160)">PTBR</span> offset by the page number for i. This task requires one memory access. It provides us with the frame number, which is combined with the page offset to produce the actual address. We can then access the desired place in memory. With this scheme, <span style="color:rgb(112, 48, 160)">two</span> memory accesses are needed to access data (one for the page-table entry and one for the actual data). Thus, memory access is slowed by a factor of 2, a delay that is considered intolerable under most circumstances.

- The <span style="color:rgb(112, 48, 160)">standard</span> <span style="color:rgb(112, 48, 160)">solution</span> to this problem is to use a special, small, fast-lookup hardware cache called a <span style="color:rgb(112, 48, 160)">translation look-aside buffer </span>(<span style="color:rgb(112, 48, 160)">TLB</span>). The TLB is associative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously. If the item is found, the corresponding value field is returned. The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty. To be able to execute the search within a pipeline step, however, the TLB must be kept small. It is typically between 32 and 1,024 entries in size. Some CPUs implement separate instruction and data address TLBs. That can double the number of TLB entries available, because those lookups occur in different pipeline steps. We can see in this development an example of the evolution of CPU technology: systems have evolved from having no TLBs to having multiple levels of TLBs, just as they have multiple levels of caches.
	- The <span style="color:rgb(112, 48, 160)">TLB</span> is used with <span style="color:rgb(112, 48, 160)">page tables</span> in the following way: 
		- The TLB contains only a few of the page-table entries. When a logical address is generated by the CPU, the MMU first checks if its page number is present in the TLB. 
		- If the page number is found, its frame number is immediately available and is used to access memory. 
		- As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging. 
	- If the page number is <span style="color:rgb(112, 48, 160)">not</span> <span style="color:rgb(112, 48, 160)">in the TLB</span> (known as a <span style="color:rgb(112, 48, 160)">TLB miss</span>), address translation proceeds following the steps illustrated in [[Paging|Section 9.3.1]], where a memory reference to the page table must be made. When the frame number is obtained, we can use it to access memory (<span style="color:rgb(112, 48, 160)">Figure 9.12</span>). In addition, we add the page number and frame number to the TLB, so that they will be found quickly on the next reference.
	- ![[Pasted image 20250214124814.png]]
		- Some TLBs store <span style="color:rgb(112, 48, 160)">address-space identifier</span> (<span style="color:rgb(112, 48, 160)">ASIDs</span>) in each TLB entry. <span style="color:rgb(112, 48, 160)">An ASID uniquely identifies each process and is used to provide address-space protection for that process</span>. 
		- When the TLB attempts to resolve virtual page numbers, it ensures that the ASID for the currently running process matches the ASID associated with the virtual page. 
			- If the ASIDs do not match, the attempt is treated as a TLB miss. In addition to providing address-space protection, an ASID allows the TLB to contain entries for several different processes simultaneously. 
			- If the TLB does not support separate ASIDs, then every time a new page table is selected (for instance, with each context switch), the TLB must be flushed (or erased) to ensure that the next executing process does not use the
	- The percentage of times that the page number of interest is found in the TLB is called the hit ratio. An 80-percent <span style="color:rgb(112, 48, 160)">hit ratio</span>, for example, means that we find the desired page number in the TLB 80 percent of the time. If it takes 10 nanoseconds to access memory, then a mapped-memory access takes 10 nanoseconds when the page number is in the TLB. If we fail to find the page number in the TLB then we must first access memory for the page table and frame number (10 nanoseconds) and then access the desired byte in memory (10 nanoseconds), for a total of <span style="color:rgb(112, 48, 160)">20</span> nanoseconds. (We are assuming that a page-table lookup takes only one memory access, but it can take more, as we shall see.) To find the effective memory-access time, we weight the case by its probability:
		- `effective access time = 0.80 × 10 + 0.20 × 20 = 12 nanoseconds`
	- In this example, we suffer a <span style="color:rgb(112, 48, 160)">20-percent slowdown</span> in average memory-access time (from 10 to 12 nanoseconds). For a 99-percent hit ratio, which is much more realistic, we have
		- `effective access time = 0.99 × 10 + 0.01 × 20 = 10.1 nanoseconds`